Funciones de Activaci贸n para Redes Neuronales 
 Autor: Alejandro Garcia
 Fecha: 7 de marzo
 Instagram: @ale_garcia454

 Este proyecto implementa y analiza diferentes funciones de activaci贸n utilizadas en redes neuronales para clasificaci贸n de d铆gitos manuscritos. Las funciones de activaci贸n juegan un papel crucial en la capacidad de la red para aprender y realizar predicciones.

Funciones de Activaci贸n Utilizadas
1. ReLU (Rectified Linear Unit)
La funci贸n ReLU es ampliamente utilizada en redes neuronales profundas debido a su simplicidad y eficiencia computacional. Permite que la red sea no lineal, mientras evita el problema del desvanecimiento del gradiente.
2. Sigmoidal
La funci贸n sigmoidal mapea cualquier valor de entrada al rango entre 0 y 1, lo que la hace 煤til en tareas de clasificaci贸n binaria, aunque puede sufrir del problema del desvanecimiento del gradiente.
3. Escal贸n (Heaviside)
La funci贸n escal贸n es una de las m谩s simples. No es diferenciable en x=0, pero es 煤til en ciertas aplicaciones de clasificaci贸n. Sin embargo, rara vez se usa hoy en d铆a debido a su falta de suavidad.
4. Gaussiana
La funci贸n gaussiana o "campana de Gauss" es 煤til cuando se requiere una activaci贸n que decaiga r谩pidamente desde el centro hacia los extremos. Se usa en redes neuronales de tipo RBF (Radial Basis Function).
5. Identidad
La funci贸n de identidad no cambia la entrada, lo que significa que simplemente pasa los valores tal cual como son. Se usa generalmente en la capa de salida de redes neuronales para regresi贸n.
6. Linear
Similar a la funci贸n de identidad, pero con un factor de escala a y un sesgo b. Se utiliza cuando la salida de la red neuronal necesita ser una combinaci贸n lineal de las entradas.
7. Sinusoidal
Esta funci贸n introduce una forma de onda sinusoidal en las activaciones. Puede ser 煤til en tareas donde se requiere modelar relaciones peri贸dicas, aunque no es muy com煤n en redes neuronales est谩ndar.
8. Tanh (Tangente Hiperb贸lica)
La funci贸n tanh es similar a la sigmoidal, pero su salida est谩 en el rango de -1 a 1. Ayuda a centrar los datos alrededor de cero, lo que puede facilitar el entrenamiento de la red.
 Caracter铆sticas
 Implementaci贸n de m煤ltiples funciones de activaci贸n.
 An谩lisis de su impacto en el rendimiento de una red neuronal.
 Comparaci贸n entre funciones lineales y no lineales.
 Aplicaci贸n pr谩ctica en clasificaci贸n de d铆gitos manuscritos utilizando el conjunto de datos MNIST.

 Requisitos
Aseg煤rate de tener instaladas las siguientes librer铆as antes de ejecutar el c贸digo:

bash
Copiar
Editar
pip install numpy keras tensorflow matplotlib
 Uso
Para ejecutar el c贸digo, simplemente corre el siguiente comando en tu terminal:

bash
Copiar
Editar
python main.py
Esto entrenar谩 la red neuronal con el dataset MNIST y aplicar谩 las funciones de activaci贸n mencionadas.

 Estructura del Proyecto
La estructura del proyecto es la siguiente:

 main/
 src/
  pycache/
  neural_network_keras.py
 .gitignore
 README.md
 requirements.txt

Autor
 Desarrollado por Alejandro Garcia.
